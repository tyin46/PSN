"""
LLM vs Human Decision Comparison
ä½¿ç”¨çœŸå®APIè°ƒç”¨è·å–LLMå†³ç­–ï¼Œä¸äººç±»å¹³å‡è¡Œä¸ºè¿›è¡Œè¯¦ç»†å¯¹æ¯”åˆ†æ
"""

import json
import time
import random
from pathlib import Path
from typing import Dict, List, Tuple
from dataclasses import dataclass, asdict
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import os
import sys

# Add project root to path
_ROOT = Path(__file__).resolve().parent
if str(_ROOT) not in sys.path:
    sys.path.insert(0, str(_ROOT))

try:
    from dotenv import load_dotenv
    load_dotenv(_ROOT / ".env")
    from openai import OpenAI
    HAS_OPENAI = True
except:
    HAS_OPENAI = False
    OpenAI = None

@dataclass
class DecisionComparison:
    """å•æ¬¡å†³ç­–çš„å¯¹æ¯”æ•°æ®"""
    trial_id: int
    pumps: int
    state_description: str
    llm_decision: str
    llm_reasoning: str
    human_typical_decision: str
    human_decision_probability: float
    decision_match: bool
    risk_level: str  # "low", "medium", "high"

class LLMHumanComparator:
    """LLMå†³ç­–ä¸äººç±»å†³ç­–çš„è¯¦ç»†å¯¹æ¯”åˆ†æå™¨"""
    
    def __init__(self):
        self.output_dir = Path("llm_human_comparison")
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialize OpenAI client
        self.client = None
        if HAS_OPENAI:
            try:
                self.client = OpenAI()
                print("âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                print("è¯·æ£€æŸ¥OPENAI_API_KEYç¯å¢ƒå˜é‡è®¾ç½®")
                
        # åŠ è½½äººæ ¼æ–‡ä»¶
        self.risk_taker_persona = self._load_persona("risk_taker.txt")
        self.cautious_persona = self._load_persona("cautious_thinker.txt")
        
        # äººç±»è¡Œä¸ºåŸºçº¿æ•°æ®ï¼ˆæ¥è‡ªå¿ƒç†å­¦ç ”ç©¶ï¼‰
        self.human_baseline = {
            'avg_pumps': 16.5,
            'explosion_rate': 0.37,
            'avg_adjusted_pumps': 20.2,
            'typical_stopping_points': [8, 12, 16, 20, 24],  # å¸¸è§åœæ­¢ç‚¹
            'risk_aversion_curve': self._generate_human_risk_curve()
        }
        
    def _load_persona(self, filename: str) -> str:
        """åŠ è½½äººæ ¼æ–‡ä»¶"""
        filepath = _ROOT / filename
        if filepath.exists():
            return filepath.read_text(encoding='utf-8')
        else:
            print(f"âš ï¸ äººæ ¼æ–‡ä»¶ {filename} æœªæ‰¾åˆ°")
            return f"You are a {filename.replace('.txt', '').replace('_', ' ')} persona."
            
    def _generate_human_risk_curve(self) -> Dict[int, float]:
        """ç”Ÿæˆäººç±»é£é™©è§„é¿æ›²çº¿ï¼ˆåŸºäºå¿ƒç†å­¦ç ”ç©¶æ•°æ®ï¼‰"""
        # åŸºäºå®é™…BARTç ”ç©¶çš„äººç±»å†³ç­–æ¦‚ç‡
        risk_curve = {}
        for pumps in range(1, 33):
            if pumps <= 5:
                pump_prob = 0.9  # æ—©æœŸé«˜æ¦‚ç‡ç»§ç»­
            elif pumps <= 10:
                pump_prob = 0.8 - (pumps - 5) * 0.1  # é€æ¸é™ä½
            elif pumps <= 15:
                pump_prob = 0.3 - (pumps - 10) * 0.05  # åŠ é€Ÿé™ä½
            elif pumps <= 20:
                pump_prob = 0.05 - (pumps - 15) * 0.01  # å¾ˆä½æ¦‚ç‡
            else:
                pump_prob = 0.01  # æä½æ¦‚ç‡
            risk_curve[pumps] = max(0.01, pump_prob)
        return risk_curve
        
    def get_human_typical_decision(self, pumps: int) -> Tuple[str, float]:
        """è·å–äººç±»åœ¨ç‰¹å®šçŠ¶æ€ä¸‹çš„å…¸å‹å†³ç­–å’Œæ¦‚ç‡"""
        pump_probability = self.human_baseline['risk_aversion_curve'].get(pumps, 0.01)
        
        if pump_probability > 0.5:
            typical_decision = "PUMP"
        else:
            typical_decision = "COLLECT"
            
        return typical_decision, pump_probability
        
    def get_llm_decision_with_reasoning(self, persona_text: str, state_desc: str, 
                                      persona_name: str) -> Tuple[str, str]:
        """è·å–LLMçš„å†³ç­–å’Œæ¨ç†è¿‡ç¨‹"""
        
        if not self.client:
            # ç®€å•æ¨¡æ‹Ÿå›é€€
            if "risk" in persona_name.lower():
                return "PUMP", "æ¨¡æ‹Ÿå›å¤ï¼šä½œä¸ºé£é™©æ‰¿æ‹…è€…ï¼Œæˆ‘é€‰æ‹©ç»§ç»­æ‰“æ°”è·å–æ›´å¤šå¥–åŠ±ã€‚"
            else:
                return "COLLECT", "æ¨¡æ‹Ÿå›å¤ï¼šä½œä¸ºè°¨æ…è€…ï¼Œæˆ‘é€‰æ‹©æ”¶é›†å½“å‰å¥–åŠ±ä»¥é¿å…æŸå¤±ã€‚"
        
        decision_prompt = f"""{persona_text}

å½“å‰æƒ…å†µï¼š{state_desc}

è¯·åšå‡ºå†³ç­–å¹¶è§£é‡Šä½ çš„æ¨ç†è¿‡ç¨‹ã€‚

è¾“å‡ºæ ¼å¼ï¼š
å†³ç­–ï¼šPUMP æˆ– COLLECT
æ¨ç†ï¼š[ä½ çš„è¯¦ç»†æ¨ç†è¿‡ç¨‹ï¼Œ2-3å¥è¯]

ä¾‹å¦‚ï¼š
å†³ç­–ï¼šPUMP
æ¨ç†ï¼šè™½ç„¶é£é™©åœ¨å¢åŠ ï¼Œä½†å½“å‰å¥–åŠ±è¿˜ä¸å¤Ÿé«˜ï¼Œæˆ‘æ„¿æ„æ‰¿æ‹…é¢å¤–é£é™©æ¥è·å–æ›´å¤§æ”¶ç›Šã€‚åŸºäºè¿‡å¾€ç»éªŒï¼Œè¿™ä¸ªé˜¶æ®µçˆ†ç‚¸æ¦‚ç‡ä»ç„¶è¾ƒä½ã€‚"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": decision_prompt}],
                temperature=0.7,
                max_tokens=150,
                timeout=15
            )
            
            content = response.choices[0].message.content.strip()
            
            # è§£æå†³ç­–å’Œæ¨ç†
            lines = content.split('\n')
            decision = "COLLECT"  # é»˜è®¤å€¼
            reasoning = "æ— æ³•è§£ææ¨ç†è¿‡ç¨‹"
            
            for line in lines:
                if line.startswith("å†³ç­–ï¼š") or line.startswith("Decision:"):
                    if "PUMP" in line.upper():
                        decision = "PUMP"
                    else:
                        decision = "COLLECT"
                elif line.startswith("æ¨ç†ï¼š") or line.startswith("Reasoning:"):
                    reasoning = line.split("ï¼š", 1)[-1].strip()
                    
            return decision, reasoning
            
        except Exception as e:
            print(f"âš ï¸ LLMæŸ¥è¯¢å¤±è´¥: {e}")
            # å›é€€å†³ç­–
            if "risk" in persona_name.lower():
                return "PUMP", "APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é£é™©æ‰¿æ‹…è€…é»˜è®¤ç­–ç•¥ã€‚"
            else:
                return "COLLECT", "APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨è°¨æ…è€…é»˜è®¤ç­–ç•¥ã€‚"
                
    def run_detailed_comparison(self, n_scenarios: int = 20) -> List[DecisionComparison]:
        """è¿è¡Œè¯¦ç»†çš„LLMä¸äººç±»å†³ç­–å¯¹æ¯”"""
        
        if not self.client:
            print("âŒ æ— æ³•è¿›è¡ŒçœŸå®LLMå¯¹æ¯”ï¼Œéœ€è¦æœ‰æ•ˆçš„OpenAI APIå¯†é’¥")
            return []
            
        print(f"ğŸ” å¼€å§‹LLMä¸äººç±»å†³ç­–è¯¦ç»†å¯¹æ¯”åˆ†æ")
        print(f"ğŸ“Š å°†åˆ†æ {n_scenarios} ä¸ªå†³ç­–åœºæ™¯")
        
        comparisons = []
        personas = [
            ("é£é™©æ‰¿æ‹…è€…", self.risk_taker_persona),
            ("è°¨æ…æ€è€ƒè€…", self.cautious_persona)
        ]
        
        scenario_id = 0
        
        for persona_name, persona_text in personas:
            print(f"\nğŸ­ åˆ†æäººæ ¼: {persona_name}")
            
            # ä¸ºæ¯ä¸ªäººæ ¼ç”Ÿæˆå¤šä¸ªå†³ç­–åœºæ™¯
            for pumps in [3, 7, 12, 17, 22, 27]:  # ä¸åŒé£é™©æ°´å¹³
                scenario_id += 1
                
                # ç”ŸæˆçŠ¶æ€æè¿°
                earned = pumps * 0.05
                risk_level = "ä½" if pumps <= 8 else "ä¸­" if pumps <= 16 else "é«˜"
                
                state_desc = f"""æ°”çƒä»»åŠ¡çŠ¶æ€ï¼š
- å½“å‰å·²æ‰“æ°” {pumps} æ¬¡
- å·²è·å¾—å¥–åŠ±ï¼š${earned:.2f}
- é£é™©æ°´å¹³ï¼š{risk_level}
- ç»§ç»­æ‰“æ°”å¯è·å¾—æ›´å¤šå¥–åŠ±ï¼Œä½†æ°”çƒå¯èƒ½çˆ†ç‚¸å¯¼è‡´æœ¬è½®å¥–åŠ±æ¸…é›¶
- é€‰æ‹©æ”¶é›†å¯ä¿ç•™å½“å‰å¥–åŠ±"""

                print(f"  ğŸ“ åœºæ™¯ {scenario_id}: {pumps} æ¬¡æ‰“æ°” (é£é™©æ°´å¹³: {risk_level})")
                
                # è·å–LLMå†³ç­–
                llm_decision, llm_reasoning = self.get_llm_decision_with_reasoning(
                    persona_text, state_desc, persona_name)
                
                # è·å–äººç±»å…¸å‹å†³ç­–
                human_decision, human_prob = self.get_human_typical_decision(pumps)
                
                # åˆ¤æ–­æ˜¯å¦åŒ¹é…
                decision_match = llm_decision == human_decision
                
                # é£é™©æ°´å¹³åˆ†ç±»
                risk_category = "ä½é£é™©" if pumps <= 8 else "ä¸­é£é™©" if pumps <= 16 else "é«˜é£é™©"
                
                comparison = DecisionComparison(
                    trial_id=scenario_id,
                    pumps=pumps,
                    state_description=state_desc,
                    llm_decision=llm_decision,
                    llm_reasoning=llm_reasoning,
                    human_typical_decision=human_decision,
                    human_decision_probability=human_prob,
                    decision_match=decision_match,
                    risk_level=risk_category
                )
                
                comparisons.append(comparison)
                
                print(f"    ğŸ¤– LLMå†³ç­–: {llm_decision}")
                print(f"    ğŸ‘¥ äººç±»å…¸å‹: {human_decision} (æ¦‚ç‡: {human_prob:.2f})")
                print(f"    âœ… åŒ¹é…: {'æ˜¯' if decision_match else 'å¦'}")
                
        return comparisons
        
    def analyze_and_visualize(self, comparisons: List[DecisionComparison]):
        """åˆ†æå¯¹æ¯”ç»“æœå¹¶ç”Ÿæˆå¯è§†åŒ–"""
        
        if not comparisons:
            print("âŒ æ²¡æœ‰å¯¹æ¯”æ•°æ®å¯ä¾›åˆ†æ")
            return
            
        print(f"\nğŸ“Š å¼€å§‹åˆ†æ {len(comparisons)} ä¸ªå†³ç­–å¯¹æ¯”...")
        
        # è½¬æ¢ä¸ºDataFrameä¾¿äºåˆ†æ
        df = pd.DataFrame([asdict(comp) for comp in comparisons])
        
        # åˆ›å»ºç»¼åˆå¯è§†åŒ–
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        fig.suptitle('LLM vs äººç±»å†³ç­–è¯¦ç»†å¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. å†³ç­–åŒ¹é…ç‡
        ax = axes[0, 0]
        match_rate = df['decision_match'].mean()
        risk_levels = df['risk_level'].unique()
        match_by_risk = df.groupby('risk_level')['decision_match'].mean()
        
        bars = ax.bar(range(len(risk_levels)), [match_by_risk[level] for level in risk_levels], 
                     color=['green', 'orange', 'red'], alpha=0.7)
        ax.axhline(match_rate, color='blue', linestyle='--', linewidth=2, 
                  label=f'æ€»ä½“åŒ¹é…ç‡: {match_rate:.2f}')
        ax.set_xticks(range(len(risk_levels)))
        ax.set_xticklabels(risk_levels)
        ax.set_ylabel('å†³ç­–åŒ¹é…ç‡')
        ax.set_title('ä¸åŒé£é™©æ°´å¹³ä¸‹çš„å†³ç­–åŒ¹é…ç‡')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. LLM vs äººç±»å†³ç­–åˆ†å¸ƒ
        ax = axes[0, 1]
        llm_decisions = df['llm_decision'].value_counts()
        human_decisions = df['human_typical_decision'].value_counts()
        
        x = np.arange(len(['PUMP', 'COLLECT']))
        width = 0.35
        
        ax.bar(x - width/2, [llm_decisions.get('PUMP', 0), llm_decisions.get('COLLECT', 0)], 
               width, label='LLMå†³ç­–', alpha=0.8, color='skyblue')
        ax.bar(x + width/2, [human_decisions.get('PUMP', 0), human_decisions.get('COLLECT', 0)], 
               width, label='äººç±»å…¸å‹å†³ç­–', alpha=0.8, color='lightcoral')
        
        ax.set_xlabel('å†³ç­–ç±»å‹')
        ax.set_ylabel('é¢‘æ¬¡')
        ax.set_title('LLM vs äººç±»å†³ç­–åˆ†å¸ƒ')
        ax.set_xticks(x)
        ax.set_xticklabels(['PUMP', 'COLLECT'])
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 3. é£é™©æ°´å¹³ä¸å†³ç­–å…³ç³»
        ax = axes[0, 2]
        risk_decision_crosstab = pd.crosstab(df['risk_level'], df['llm_decision'], normalize='index')
        risk_decision_crosstab.plot(kind='bar', ax=ax, color=['lightblue', 'lightgreen'])
        ax.set_title('LLMåœ¨ä¸åŒé£é™©æ°´å¹³ä¸‹çš„å†³ç­–æ¨¡å¼')
        ax.set_xlabel('é£é™©æ°´å¹³')
        ax.set_ylabel('å†³ç­–æ¯”ä¾‹')
        ax.legend(title='LLMå†³ç­–')
        ax.grid(True, alpha=0.3)
        plt.setp(ax.get_xticklabels(), rotation=45)
        
        # 4. äººç±»å†³ç­–æ¦‚ç‡åˆ†å¸ƒ
        ax = axes[1, 0]
        ax.scatter(df['pumps'], df['human_decision_probability'], 
                  c=['red' if d == 'PUMP' else 'blue' for d in df['human_typical_decision']], 
                  s=80, alpha=0.7, edgecolors='black')
        ax.set_xlabel('æ‰“æ°”æ¬¡æ•°')
        ax.set_ylabel('äººç±»PUMPå†³ç­–æ¦‚ç‡')
        ax.set_title('äººç±»é£é™©è§„é¿æ›²çº¿')
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ è¶‹åŠ¿çº¿
        z = np.polyfit(df['pumps'], df['human_decision_probability'], 2)
        p = np.poly1d(z)
        x_trend = np.linspace(df['pumps'].min(), df['pumps'].max(), 100)
        ax.plot(x_trend, p(x_trend), "r--", alpha=0.8, linewidth=2, label='è¶‹åŠ¿çº¿')
        ax.legend()
        
        # 5. å†³ç­–ä¸€è‡´æ€§çƒ­å›¾
        ax = axes[1, 1]
        
        # åˆ›å»ºå†³ç­–å¯¹æ¯”çŸ©é˜µ
        decision_matrix = np.zeros((2, 2))
        for _, row in df.iterrows():
            llm_idx = 0 if row['llm_decision'] == 'PUMP' else 1
            human_idx = 0 if row['human_typical_decision'] == 'PUMP' else 1
            decision_matrix[llm_idx, human_idx] += 1
            
        im = ax.imshow(decision_matrix, cmap='Blues', aspect='auto')
        ax.set_xticks([0, 1])
        ax.set_xticklabels(['äººç±»PUMP', 'äººç±»COLLECT'])
        ax.set_yticks([0, 1])
        ax.set_yticklabels(['LLM PUMP', 'LLM COLLECT'])
        ax.set_title('LLM vs äººç±»å†³ç­–æ··æ·†çŸ©é˜µ')
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(2):
            for j in range(2):
                ax.text(j, i, f'{int(decision_matrix[i, j])}',
                       ha="center", va="center", color="white", fontweight='bold')
        
        plt.colorbar(im, ax=ax)
        
        # 6. è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        ax = axes[1, 2]
        ax.axis('off')
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        total_comparisons = len(comparisons)
        matches = sum(1 for c in comparisons if c.decision_match)
        match_rate = matches / total_comparisons
        
        pump_decisions_llm = sum(1 for c in comparisons if c.llm_decision == 'PUMP')
        pump_decisions_human = sum(1 for c in comparisons if c.human_typical_decision == 'PUMP')
        
        stats_text = f"""ğŸ“ˆ è¯¦ç»†ç»Ÿè®¡æ•°æ®

ğŸ” æ€»å¯¹æ¯”åœºæ™¯æ•°: {total_comparisons}
âœ… å†³ç­–åŒ¹é…æ•°: {matches}
ğŸ“Š æ€»ä½“åŒ¹é…ç‡: {match_rate:.2%}

ğŸ¤– LLMå†³ç­–ç»Ÿè®¡:
   PUMP: {pump_decisions_llm} ({pump_decisions_llm/total_comparisons:.1%})
   COLLECT: {total_comparisons-pump_decisions_llm} ({(total_comparisons-pump_decisions_llm)/total_comparisons:.1%})

ğŸ‘¥ äººç±»å…¸å‹å†³ç­–:
   PUMP: {pump_decisions_human} ({pump_decisions_human/total_comparisons:.1%})
   COLLECT: {total_comparisons-pump_decisions_human} ({(total_comparisons-pump_decisions_human)/total_comparisons:.1%})

ğŸ¯ é£é™©æ°´å¹³åŒ¹é…ç‡:
   ä½é£é™©: {df[df['risk_level']=='ä½é£é™©']['decision_match'].mean():.1%}
   ä¸­é£é™©: {df[df['risk_level']=='ä¸­é£é™©']['decision_match'].mean():.1%}
   é«˜é£é™©: {df[df['risk_level']=='é«˜é£é™©']['decision_match'].mean():.1%}"""

        ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, fontsize=11,
               verticalalignment='top', fontfamily='monospace',
               bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'llm_human_detailed_comparison.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
        
        # ä¿å­˜è¯¦ç»†å¯¹æ¯”æ•°æ®
        self.save_detailed_results(comparisons, df)
        
    def save_detailed_results(self, comparisons: List[DecisionComparison], df: pd.DataFrame):
        """ä¿å­˜è¯¦ç»†çš„å¯¹æ¯”ç»“æœ"""
        
        # ä¿å­˜åŸå§‹å¯¹æ¯”æ•°æ®
        timestamp = int(time.time())
        results_file = self.output_dir / f"llm_human_comparison_{timestamp}.json"
        
        comparison_data = {
            'metadata': {
                'timestamp': timestamp,
                'total_comparisons': len(comparisons),
                'personas_tested': ['é£é™©æ‰¿æ‹…è€…', 'è°¨æ…æ€è€ƒè€…'],
                'human_baseline': self.human_baseline
            },
            'comparisons': [asdict(comp) for comp in comparisons]
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜CSVæ ¼å¼ä¾¿äºè¿›ä¸€æ­¥åˆ†æ
        csv_file = self.output_dir / f"llm_human_comparison_{timestamp}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜:")
        print(f"   JSON: {results_file}")
        print(f"   CSV:  {csv_file}")
        
    def print_detailed_analysis(self, comparisons: List[DecisionComparison]):
        """æ‰“å°è¯¦ç»†åˆ†ææŠ¥å‘Š"""
        
        print("\n" + "="*80)
        print("LLMä¸äººç±»å†³ç­–è¯¦ç»†å¯¹æ¯”åˆ†ææŠ¥å‘Š")
        print("="*80)
        
        total = len(comparisons)
        matches = sum(1 for c in comparisons if c.decision_match)
        
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»å¯¹æ¯”åœºæ™¯: {total}")
        print(f"   å†³ç­–åŒ¹é…: {matches} ({matches/total:.1%})")
        print(f"   å†³ç­–ä¸åŒ¹é…: {total-matches} ({(total-matches)/total:.1%})")
        
        print(f"\nğŸ­ æŒ‰äººæ ¼ç±»å‹åˆ†æ:")
        persona_stats = {}
        for comp in comparisons:
            # ç®€å•æ ¹æ®trial_idåˆ¤æ–­äººæ ¼ï¼ˆå‰åŠéƒ¨åˆ†æ˜¯é£é™©æ‰¿æ‹…è€…ï¼‰
            persona = "é£é™©æ‰¿æ‹…è€…" if comp.trial_id <= total//2 else "è°¨æ…æ€è€ƒè€…"
            if persona not in persona_stats:
                persona_stats[persona] = {'total': 0, 'matches': 0}
            persona_stats[persona]['total'] += 1
            if comp.decision_match:
                persona_stats[persona]['matches'] += 1
                
        for persona, stats in persona_stats.items():
            match_rate = stats['matches'] / stats['total']
            print(f"   {persona}: {stats['matches']}/{stats['total']} ({match_rate:.1%})")
        
        print(f"\nğŸ¯ æŒ‰é£é™©æ°´å¹³åˆ†æ:")
        risk_stats = {}
        for comp in comparisons:
            risk = comp.risk_level
            if risk not in risk_stats:
                risk_stats[risk] = {'total': 0, 'matches': 0}
            risk_stats[risk]['total'] += 1
            if comp.decision_match:
                risk_stats[risk]['matches'] += 1
                
        for risk, stats in risk_stats.items():
            match_rate = stats['matches'] / stats['total']
            print(f"   {risk}: {stats['matches']}/{stats['total']} ({match_rate:.1%})")
        
        print(f"\nğŸ’¡ å…³é”®å‘ç°:")
        
        # LLMå†³ç­–å€¾å‘
        llm_pumps = sum(1 for c in comparisons if c.llm_decision == 'PUMP')
        human_pumps = sum(1 for c in comparisons if c.human_typical_decision == 'PUMP')
        
        print(f"   â€¢ LLMå€¾å‘: {llm_pumps/total:.1%} é€‰æ‹©PUMP")
        print(f"   â€¢ äººç±»å€¾å‘: {human_pumps/total:.1%} é€‰æ‹©PUMP")
        
        if llm_pumps > human_pumps:
            print(f"   â€¢ LLMæ¯”äººç±»æ›´æ¿€è¿› (+{(llm_pumps-human_pumps)/total:.1%})")
        else:
            print(f"   â€¢ LLMæ¯”äººç±»æ›´ä¿å®ˆ ({(llm_pumps-human_pumps)/total:.1%})")
        
        # å±•ç¤ºä¸€äº›å…·ä½“çš„å†³ç­–æ¨ç†
        print(f"\nğŸ§  LLMå†³ç­–æ¨ç†ç¤ºä¾‹:")
        for i, comp in enumerate(comparisons[:3]):
            match_status = "âœ…åŒ¹é…" if comp.decision_match else "âŒä¸åŒ¹é…"
            print(f"\n   ç¤ºä¾‹ {i+1} - {comp.pumps}æ¬¡æ‰“æ°” ({comp.risk_level}) {match_status}")
            print(f"   LLMå†³ç­–: {comp.llm_decision}")
            print(f"   äººç±»å…¸å‹: {comp.human_typical_decision}")
            print(f"   LLMæ¨ç†: {comp.llm_reasoning}")

def main():
    """è¿è¡ŒLLMä¸äººç±»å†³ç­–å¯¹æ¯”åˆ†æ"""
    
    print("ğŸš€ LLMä¸äººç±»å†³ç­–è¯¦ç»†å¯¹æ¯”åˆ†æ")
    print("="*50)
    
    comparator = LLMHumanComparator()
    
    if not comparator.client:
        print("âŒ æ— æ³•è¿è¡Œï¼Œéœ€è¦æœ‰æ•ˆçš„OpenAI APIå¯†é’¥")
        print("è¯·ç¡®ä¿OPENAI_API_KEYç¯å¢ƒå˜é‡å·²æ­£ç¡®è®¾ç½®")
        return
    
    # è¿è¡Œè¯¦ç»†å¯¹æ¯”
    comparisons = comparator.run_detailed_comparison(n_scenarios=20)
    
    if comparisons:
        # åˆ†æå’Œå¯è§†åŒ–
        comparator.analyze_and_visualize(comparisons)
        
        # æ‰“å°è¯¦ç»†æŠ¥å‘Š
        comparator.print_detailed_analysis(comparisons)
        
        print(f"\nğŸ‰ LLMä¸äººç±»å†³ç­–å¯¹æ¯”åˆ†æå®Œæˆ!")
        print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨: llm_human_comparison/llm_human_detailed_comparison.png")
        print(f"ğŸ“ è¯¦ç»†æ•°æ®: llm_human_comparison/ ç›®å½•")
    
    else:
        print("âŒ æœªèƒ½ç”Ÿæˆå¯¹æ¯”æ•°æ®")

if __name__ == "__main__":
    main()